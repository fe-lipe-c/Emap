# O Brother, How Far Art Thou?

## General Guidance

1. State and prove all non-trivial mathematical results necessary to substantiate your arguments;
2. Do not forget to add appropriate scholarly references at the end of the document;
3. Mathematical expressions also receive punctuation;

## Background

A large portion of the content of this course is concerned with computing high-dimensional integrals via simulation. Today you will be introduced to a simple-looking problem with a complicated closed-form solution and one we can approach using simulation.

Suppose you have a disc $C _{R}$ of radius $R$. Take $p = (p _{x},p _{y})$ and $q= (q _{x}, q _{y}) \in C _{R}$ two points in the disc. Consider the Euclidean distance between $p$ and $q$, $||p -q|| = \sqrt{(p _{x} - q _{x})^{2} + (p _{y}- q _{y})^{2}} = |p - q|$. 

**Problem A:** What is the average distance between pairs of points in $C _{R}$ if they are picked uniformly at random?

## Part I: nuts and bolts

1. To start building intuition, let's solve a related but much simpler problem. Consider an interval $[0,s]$, with $s >0$ and take $x _{1}, x _{2} \in [0,s]$ uniformly at random. Show that the average distance between $x _{1}$ and $x _{2}$ is $s /3$.

`Solution`

We have that $X _{1}, X _{2} \sim \mathcal{U}(0,s)$. Then, $f _{X _{1}}(x _{1}) = f _{X _{2}}(x _{2}) = \frac{1}{s}$, if $x _{1}, x _{2} \in [0,s]$ and zero otherwise. Define 
$$
g(x _{1}, x _{2}) = |x _{1} - x _{2}| =\begin{cases}
 x _{1} - x _{2}, \qquad \text{if } \quad x _{1} \geq x _{2} \\
 x _{2} - x _{1}, \qquad \text{if } \quad x _{1} < x _{2}
\end{cases}
$$
By the independence of $X _{1}$ and $X _{2}$, we have that $f _{X _{1}, X _{2}}(x _{1},x _{2}) = f _{X _{1}}(x _{1})f _{X _{2}}(x _{2}) = \frac{1}{s ^{2}}$.

Therefore, 
$$
\begin{align*}
	\mathbb{E}_{X _{1},X _{2}}[g(x _{1}, x _{2})] &= \int_{0}^{s} \int_{0}^{s} g(x _{1}, x _{2})f _{X _{1}, X _{2}}(x _{1},x _{2})dx _{1}dx _{2} \\
	&= \int_{0}^{s} \int_{0}^{s} g(x _{1}, x _{2})\frac{1}{s ^{2}}dx _{1}dx _{2} = \frac{1}{s ^{2}}\int_{0}^{s} \int_{0}^{s} |x _{1} - x _{2}|dx _{1}dx _{2} \\
	&= \frac{1}{s ^{2}}\int_{0}^{s}\int_{0}^{x _{1}} |x _{1} - x _{2}|dx _{1}dx _{2} + \frac{1}{s ^{2}}\int_{0}^{s}\int_{x _{1}}^{s} |x _{1} - x _{2}|dx _{1}dx _{2} \\
	&= \frac{1}{s ^{2}}\int_{0}^{s}\int_{0}^{x _{2}} (x _{2}- x _{1})dx _{1}dx _{2} + \frac{1}{s ^{2}}\int_{0}^{s}\int_{x _{2}}^{s} (x _{1} -x _{2})dx _{1}dx _{2} \\
	&= \frac{1}{s ^{2}} \int_{0}^{s} \frac{x ^{2}}{2}d x _{2} + \frac{1}{s ^{2}} \int_{0}^{s}\frac{(s - x _{2})^{2}}{2} dx _{2} = \frac{1}{s ^{2}}\left[\frac{s ^{3}}{6}\right] + \frac{1}{s ^{2}} \left[\frac{s ^{3}}{6}\right]\\
	&=\frac{s}{3}
\end{align*}
$$
----------

2. Show that Problem A is equivalent to computing
$$
\begin{equation*}
	I = \frac{1}{\pi ^{2} R ^{4}} \int_{0}^{R}\int_{0}^{R}\int_{0}^{2 \pi}\int_{0}^{2 \pi} \sqrt{r ^{2}_{1} + r ^{2}_{2} - 2 r _{1}r _{2} \cos \phi (\theta _{1}, \theta _{2})}r _{1}r _{2}d \theta _{1}d \theta _{2}dr _{1}dr _{2},
\end{equation*}
$$
where $\phi (\theta _{1}, \theta _{2})$ is the central angle between $r _{1}$ and $r _{2}$.

*Hint: Draw a picture.* 

`Solution`

We want to find $\mathbb{E} [|| p - q||] = \mathbb{E}\left[\sqrt{(p _{x}- q _{x})^{2} + (p _{y} - q _{y})^{2}}\right]$, where $p = (p _{x}, p _{y})$ and $q = (q _{x}, q _{y})$ are uniformly sampled from a disc with radius $R$. 

Note that we can rewrite $p$ and $q$ in polar coordinates:
$$
\begin{align*}
	p &= (p _{x}, p _{y}) = (r _{1}\cos \theta _{1}, r _{1}\sin \theta _{1}) \\
	q &= (q _{x}, q _{y}) = (r _{2}\cos \theta _{2}, r _{2}\sin \theta _{2})
\end{align*}
$$
If that in mind, we can consider using a transformation, by change of variables, to find the expected distance of the desired points.

----------
**Theorem 8.1.7 (Change of Variables)[Blitzstein].**
Let $X = (X _{1},\dots,X _{n})$ be a continuous random vector with joint PDF $f _{X}$. Let $g: A _{0} \to B _{0}$ be an invertible function, where $A _{0}$ and $B _{0}$ are open subsets of $\mathbb{R}^{n}$, $A _{0}$ contains the support of $X$, and $B _{0}$ is the range of $g$.

Let $Y = g (X)$, and mirror this by letting $y = g (x)$. Since $g$ is invertible, we also have $X = g ^{-1}(Y)$ and $x = g ^{-1}(y)$.

Suppose that all the partial derivatives $\frac{\partial x _{i}}{\partial y_{j}}$ exist and are continuous, so we can form the Jacobian matrix
$$
\begin{equation*}
	J =\frac{\partial x}{\partial y} \begin{pmatrix}
		\frac{\partial x _{1}}{\partial y_{1}} & \cdots & \frac{\partial x _{1}}{\partial y_{n}} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial x _{n}}{\partial y_{1}} & \cdots & \frac{\partial x _{n}}{\partial y_{n}}
	\end{pmatrix}
\end{equation*}
$$
Also assume that the determinant of this Jacobian matrix is never 0. Then the joint PDF of $Y$ is 
$$
\begin{equation*}
	f _{Y}(y) = f _{X}(g ^{-1}(y)) \cdot |\det (J)|
\end{equation*}
$$
for $y \in B _{0}$, and 0 otherwise.

----------
We have the following PDF for points $k$ in the uniform disc distribution with radius $R$:
$$
\begin{align*}
	&f(k) = \frac{1}{\pi R^{2}}\mathbb{I}_{\left\{ ||k|| \leq R \right\}}\\
	&\Downarrow \\
	&f _{X,Y}(p _{x},p _{y}) = \frac{1}{\pi R^{2}}\mathbb{I}_{\left\{ \sqrt{p _{x}^{2}+ p _{y}^{2}} \leq R \right\}}\\
\end{align*}
$$
Using the change of variables theorem, we have
$$
\begin{equation*}
	f _{\mathcal{R},\vartheta}(r,\theta) = f _{X,Y}(x,y)\cdot |\det J|
\end{equation*}
$$
where
$$
\begin{equation*}
	J =\frac{\partial (p _{x},p _{y})}{\partial (r,\theta)} =  \begin{pmatrix}
		\cos \theta _{1} &  - r _{1}\sin \theta _{1}\\
		\sin \theta _{1} & 	r _{1}\cos \theta _{1}
		\end{pmatrix}
\end{equation*}
$$
and 
$$
\begin{equation*}
	\det J = r _{1}\cos ^{2} \theta_{1} + r _{1} \sin ^{2} \theta _{1} = r _{1}
\end{equation*}
$$
Thus
$$
\begin{equation*}
	f _{\mathcal{R},\vartheta}(r,\theta) = \frac{1}{\pi R^{2}}\cdot\mathbb{I}_{\left\{0 <r \leq R \right\}}\cdot\mathbb{I}_{\left\{0<\theta \leq 2\pi \right\}}\cdot r
\end{equation*}
$$

Finally, before the calculation of the expectation, we write the distance between $p$ and $q$ in polar coordinates:
$$
\begin{align*}
	||p - q|| &= \sqrt{(p _{x}- q _{x})^{2} + (p _{y} - q _{y})^{2}} = \sqrt{(r _{1}\cos \theta _{1} - r _{2}\cos \theta _{2})^{2} + (r _{1}\sin \theta _{1} - r _{2}\sin \theta _{2})^{2}} \\
	&= \small\sqrt{(r _{1}^{2}\cos ^{2}\theta _{1} + r _{2}\cos ^{2} \theta _{2}- 2r _{1}r _{2}\cos \theta _{1}\theta _{2}) + (r _{1}^{2}\sin ^{2}\theta _{1} + r _{2}\sin ^{2} \theta _{2}- 2r _{1}r _{2}\sin \theta _{1}\theta _{2})} \\
	&= \small\sqrt{(r _{1}^{2} + r _{2}^{2} - 2r _{1}r _{2}\cos (\theta _{1} - \theta _{2}))}, \qquad \text{(since} \cos \alpha \beta + \sin \alpha \beta = \cos (\alpha -\beta)).
\end{align*}
$$
Thus, 
$$
\begin{align*}
	\mathbb{E}_{P,Q}\left[||p - q||\right] &= \int_{\mathbb{R}^{2}}\int_{\mathbb{R}^{2}} ||p - q|| \frac{1}{\pi R ^{2}}\mathbb{I}_{\left\{||p|| \leq R \right\}}\frac{1}{\pi R ^{2}}\mathbb{I}_{\left\{||q||\leq R \right\}} dp dq\\
	&\Downarrow \tiny\text{change of variables} \\
\end{align*}
$$
$$
\begin{align*}
	\scriptsize\mathbb{E}_{\mathcal{R},\vartheta}\left[||p (r _{1},\theta _{1})-q (r _{2}, \theta _{2})|| \right]&\scriptsize= \int_{0}^{2 \pi}\int_{0}^{2 \pi} \int_{0}^{R} \int_{0}^{R} \sqrt{(r _{1}^{2} + r _{2}^{2} - 2r _{1}r _{2}\cos (\theta _{1} - \theta _{2}))} f _{\mathcal{R},\varphi}(r_{1},\theta_{1})f _{\mathcal{R},\varphi}(r_{2},\theta_{2})dr _{1}dr _{2}d\theta _{1}d\theta _{2}\\
	&\scriptsize=  \int_{0}^{2 \pi}\int_{0}^{2 \pi} \int_{0}^{R} \int_{0}^{R} \sqrt{(r _{1}^{2} + r _{2}^{2} - 2r _{1}r _{2}\cos (\theta _{1} - \theta _{2}))} \frac{1}{\pi R ^{2}}r _{1}\frac{1}{\pi R ^{2}}r _{2}dr _{1}dr _{2}d\theta _{1}d\theta _{2}\\
	&\scriptsize= \frac{1}{\pi ^{2} R ^{4}} \int_{0}^{2 \pi}\int_{0}^{2 \pi} \int_{0}^{R} \int_{0}^{R} \sqrt{(r _{1}^{2} + r _{2}^{2} - 2r _{1}r _{2}\cos (\theta _{1} - \theta _{2}))} r _{1}r _{2}dr _{1}dr _{2}d\theta _{1}d\theta _{2}\\
\end{align*}
$$

----------
3. Compute $I$ in closed-form.

*Hint: Look up Crofton's mean value theorem or Crofton's formula.* 

`Solution`

First let's state Crofton's formula and mean value theorem, as described in [2]:

----------
**Theorem 2.2.4. (Crofton's Formula)** 
Let $D$ be a domain in the k-dimensional Euclidean space $\mathbb{R}^{k}$ with the volume content $V$. Consider a small increment in $D$ and let the enlarged domain be $D_{1},D \subset D_{1}$. Let the points $x_{1},\dots,x_{n}$ be independently and identically distributed in $D$. Let $P$ be an invariant probability of an event in $D$. Then
$$
\begin{equation*}
	\frac{dP}{dV} = \frac{n}{V}(P_{1}-P)
\end{equation*}
$$
where $P_{1}$ is the same probability $P$ when exactly one point is on the boundary of $D$ and the remaining $n-1$ points inside $D$.

----------

----------
**Theorem 2.2.5.** 
Let $D,V$ and $x_{1},\dots,x_{n}$ be as defined in Theorem 2.2.4. Let $\mu$ be the invariant expected value of a function of $x_{1},\dots,x_{n}$ when all the points are inside $D$ and $\mu_{1}$, the same $\mu$ when exactly one point is on the boundary an $n-1$ points inside $D$. Then
$$
\begin{equation}
	\frac{d \mu}{dV} = \frac{n}{V}(\mu_{1}-\mu) \tag{1}
\end{equation}
$$

Using the later theorem, first notice that $\mu_{1}$ can be seen as the expected distance between a point in the circumference of the disc and a random point inside the disc.

----------
In our problem we have $n = 2, V = \pi R^{2}$ and $\mu_{1}$ is the expected distance of a point in the disc to a given point in the circunference. Thus
$$
\begin{align*}
	\mu_{1} &= \mathbb{E}[||p||] = \int_{0}^{\pi} \int_{0}^{2R\sin \theta}||p (r,\theta)|| f _{\mathcal{R},\varphi}(r,\theta)dr d\theta\\
	&= \int_{0}^{\pi}\int_{0}^{2 R \sin \theta} r \frac{r}{\pi R ^{2}}dr d\theta = \frac{1}{\pi R^{2}}\int_{0}^{\pi} \int_{0}^{2R\sin \theta}r^{2} drd\theta\\
	&= \frac{1}{\pi R^{2}}\int_{0}^{\pi}\frac{(2R \sin \theta)^{3}}{3}d\theta
	= \frac{8R}{3\pi} \int_{0}^{\pi}(\sin \theta)^{3} d\theta
	= \frac{8R}{3\pi} \int_{0}^{\pi} \sin \theta (\sin \theta)^{2} d\theta\\
	&= \frac{8R}{3\pi} \int_{0}^{\pi} \sin \theta (1 - (\cos \theta)^{2}) d\theta
	= \frac{8R}{3\pi} \int_{0}^{\pi} \sin \theta- \sin \theta(\cos \theta)^{2} d\theta\\
	&= \frac{8R}{3\pi} \int_{0}^{\pi} \sin \theta d\theta-\frac{8R}{3\pi} \int_{0}^{\pi} \sin \theta(\cos \theta)^{2} d\theta\\
	&= \frac{8R}{3\pi} \left[-\cos \theta\right]_{0}^{\pi}+\frac{8R}{3\pi} \int_{0}^{\pi} u^{2} du \qquad (\text{using } u = \cos \theta \implies du = -\sin d\theta)\\
	&= \frac{8R}{3\pi} \left[1 + 1\right]+\frac{8R}{3\pi} \left[\frac{(\cos)^{3}}{3}\right]^{\pi}_{0} 
	= \frac{16 R}{3\pi} + \frac{8R}{3 \pi}\left[-\frac{1}{3}-\frac{1}{3}\right] 
	= \frac{16 R}{3\pi} - \frac{16R}{9 \pi}\\
	&= \frac{32R}{9\pi}
\end{align*}
$$
Then, $\mu_{1}$ in (1) is $\frac{32R}{9\pi}$. So far we have 
$$
\begin{equation*}
	\frac{d \mu}{dV} 
	= \frac{n}{V}(\mu_{1}-\mu) 
	= \frac{2}{\pi R^{2}}\left(\frac{32R}{9\pi}-\mu\right) 
	\implies 
  d \mu 
	= \frac{2}{\pi R^{2}}\left(\frac{32R}{9\pi}-\mu\right)dV
\end{equation*}
$$
Since $V = \pi R^{2}$, then $\frac{dV}{dR} = 2\pi R \implies dV = 2\pi R dR$. Then
$$
\begin{align*}
 d \mu &= \frac{2}{\pi R^{2}}\left(\frac{32R}{9\pi}-\mu\right) 2\pi R dR
 = \frac{4}{R}\left(\frac{32R}{9\pi}-\mu\right) dR\\
 &= \frac{128}{9\pi}dR - \frac{4\mu}{R}dR \implies d\mu + \frac{4\mu}{R}dR = \frac{128}{9\pi}dR\\
 & \frac{d\mu}{dR} + \frac{4\mu}{R} = \frac{128}{9\pi}\\
\end{align*}
$$
Multiplicating both sides by $R^{4}$ and noticing that $\frac{d (R^{4}\mu)}{dR} = 4 R^{3}\mu + R^{4}\frac{d\mu}{dR}$
$$
\begin{equation*}
	R^{4}\frac{d\mu}{dR} + 4 R^{3}\mu = \frac{128}{9\pi} R^{4} \implies \frac{d (R^{4}\mu)}{dR} = \frac{128}{9\pi}R^{4} \implies R^{4}\mu = \frac{128}{9\pi}\frac{R^{5}}{5} + c
\end{equation*}
$$
Since $\mu = 0$ when $r = 0$, then $c = 0$. Thus
$$
\begin{equation*}
	\mu = \frac{128}{45\pi}R
\end{equation*}
$$


## Part II - getting your hands dirty

Now we will move on to implementation.

**Problem B** Employ a simulation algorithm to approximate $I$. Provide point and interval estimates and give theoretical guarantees about them (consistency, coverage, etc).

1. Represent $I$ as $\int_{\mathcal{X}}\phi (x)\pi (x)dx$ and justify your choice of $\phi, \pi$ and $\mathcal{X}$. Recall that these choices are arbitrary up to a point, but they might lead to wildly different empirical performances and theoretical properties for estimators of $I$. Justify your choices in light of the method you have been given to work with. Choose wisely and be rigorous in your justifications.

justify your choice of $\phi, \pi$ and $\mathbb{X}$

`Solution`

<!-- Let's first try a simple approach. By item 2 of Part I, we know that each $p$ draw from a uniform disc can be rewritten in polar coordinates $p = (r \cos \theta, r \sin \theta)$  -->

For this part, we will use the Metropolis-Hastings method to simulate $I$. As stated in [3], this method uses a proposal distribution chosen on the space $\mathbb{X}$, with density written $q (x|x^{\prime})$, where for any $x^{\prime} \in \mathbb{X}$ we have $q (x |x^{\prime})\geq 0$ and $\int_{\mathbb{X}} q (x|x^{\prime})dx = 1$.

The Metropolis-Hastings algorithm is as follows:

----------
**Algorithm. Metropolis-Hastings** 

Starting from an arbitrary $X^{(1)}$, iterate for $t=2,3,\dots$
1. Sample $X \sim q (\cdot |X^{(t-1)})$.
2. Compute
$$
\begin{equation*}
	\alpha (X | X^{(t-1)}) = \min \left(1, \frac{\pi (X)q (X^{(t-1)}|X)}{\pi (X^{(t-1)})1 (X | X^{(t-1)})}\right)
\end{equation*}
$$
3. With probability $\alpha (X |X^{(t-1)})$, set $X^{(t)} = X$, otherwise set $X^{(t)} = X^{(t-1)}$.
----------

This method generates a Markov chain $(X^{(t)})_{t\geq 1}$ with the following transition kernel:
$$
\begin{equation*}
	K (x^{(t-1)},x^{(t)}) = \alpha (x^{(t)}| x^{(t-1)}) q (x^{(t)}|x^{(t-1)}) + (1-\kappa  (x^{(t-1)})) \delta_{x^{(t-1)}} (x^{(t)})
\end{equation*}
$$
where $\delta_{x^{(t-1)}}$ denotes the Dirac mass at $x^{(t-1)}$ and $\kappa$ is the probability of accepting a candidate, given the current state $X^{(t-1)}= x^{(t-1)}$
$$
\begin{equation*}
	\kappa (x^{(t-1)}) = \int_{\mathbb{X}} \alpha (x^{(t)}| x^{(t-1)}) q (x^{(t)}|x^{(t-1)}) dx
\end{equation*}
$$
The main purpose of building this kernel is that we can use it as a sampling proxy for the target distribution, since it is built in such way that it is $\pi$-reversible and thus admits $\pi$ as invariant distribution.

At last, before we define $\phi, \pi$ and $\mathbb{X}$, we state another theorem from [3]

----------
**Theorem 2.1.** Assume the Markov chain generated by the Metropolis-Hastings sampler is $\pi$-irreducible, then we have for any integrable function $\phi: \mathbb{X} \to \mathbb{R}$:
$$
\begin{equation*}
	\lim_{n \to \infty} \frac{1}{n} \sum_{t=1}^{n} \phi (X^{(t)}) = \int_{\mathbb{X}} \phi (x) \pi (x) dx
\end{equation*}
$$

----------
Also in [3], if the target density is bounded away from 0 and $\infty$ on compact sets, and if there exist $\delta,\epsilon >0$ such that for every $x \in \mathbb{X}$, $|x-x^{\prime}|<\delta \implies q (x|x^{\prime})\geq \epsilon$, then it can be shown that the chain is $\pi$-irreducible.

Finally, since the kernel that we will construct have theoretical guaranties that its sample trajectory will have a distribution $\pi$, we define $I$ in the same way as in part I: 
1. $\mathbb{X}= [0,2\pi]^{2}\times [0,R]^{2}$, 
2. $\pi (r,\theta) = f (r_{1},\theta_{1}) \cdot f (r_{2},\theta_{2}) = \frac{r_{1} r_{2}}{\pi^{2} R^{4}}$
3. $\phi (r,\theta) = ||p(r_{1},\theta_{1})- q(r_{2},\theta_{2})||$
