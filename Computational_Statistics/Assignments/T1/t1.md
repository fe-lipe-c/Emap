# O Brother, How Far Art Thou?

## General Guidance

1. State and prove all non-trivial mathematical results necessary to substantiate your arguments;
2. Do not forget to add appropriate scholarly references at the end of the document;
3. Mathematical expressions also receive punctuation;

## Background

A large portion of the content of this course is concerned with computing high-dimensional integrals via simulation. Today you will be introduced to a simple-looking problem with a complicated closed-form solution and one we can approach using simulation.

Suppose you have a disc $C _{R}$ of radius $R$. Take $p = (p _{x},p _{y})$ and $q= (q _{x}, q _{y}) \in C _{R}$ two points in the disc. Consider the Euclidean distance between $p$ and $q$, $||p -q|| = \sqrt{(p _{x} - q _{x})^{2} + (p _{y}- q _{y})^{2}} = |p - q|$. 

**Problem A:** What is the average distance between pairs of points in $C _{R}$ if they are picked uniformly at random?

## Part I: nuts and bolts

1. To start building intuition, let's solve a related but much simpler problem. Consider an interval $[0,s]$, with $s >0$ and take $x _{1}, x _{2} \in [0,s]$ uniformly at random. Show that the average distance between $x _{1}$ and $x _{2}$ is $s /3$.

`Solution`

We have that $X _{1}, X _{2} \sim \mathcal{U}(0,s)$. Then, $f _{X _{1}}(x _{1}) = f _{X _{2}}(x _{2}) = \frac{1}{s}$, if $x _{1}, x _{2} \in [0,s]$ and zero otherwise. Define 
$$
g(x _{1}, x _{2}) = |x _{1} - x _{2}| =\begin{cases}
 x _{1} - x _{2}, \qquad \text{if } \quad x _{1} \geq x _{2} \\
 x _{2} - x _{1}, \qquad \text{if } \quad x _{1} < x _{2}
\end{cases}
$$
By the independence of $X _{1}$ and $X _{2}$, we have that $f _{X _{1}, X _{2}}(x _{1},x _{2}) = f _{X _{1}}(x _{1})f _{X _{2}}(x _{2}) = \frac{1}{s ^{2}}$.

Therefore, 
$$
\begin{align*}
	\mathbb{E}_{X _{1},X _{2}}[g(x _{1}, x _{2})] &= \int_{0}^{s} \int_{0}^{s} g(x _{1}, x _{2})f _{X _{1}, X _{2}}(x _{1},x _{2})dx _{1}dx _{2} \\
	&= \int_{0}^{s} \int_{0}^{s} g(x _{1}, x _{2})\frac{1}{s ^{2}}dx _{1}dx _{2} = \frac{1}{s ^{2}}\int_{0}^{s} \int_{0}^{s} |x _{1} - x _{2}|dx _{1}dx _{2} \\
	&= \frac{1}{s ^{2}}\int_{0}^{s}\int_{0}^{x _{1}} |x _{1} - x _{2}|dx _{1}dx _{2} + \frac{1}{s ^{2}}\int_{0}^{s}\int_{x _{1}}^{s} |x _{1} - x _{2}|dx _{1}dx _{2} \\
	&= \frac{1}{s ^{2}}\int_{0}^{s}\int_{0}^{x _{2}} (x _{2}- x _{1})dx _{1}dx _{2} + \frac{1}{s ^{2}}\int_{0}^{s}\int_{x _{2}}^{s} (x _{1} -x _{2})dx _{1}dx _{2} \\
	&= \frac{1}{s ^{2}} \int_{0}^{s} \frac{x ^{2}}{2}d x _{2} + \frac{1}{s ^{2}} \int_{0}^{s}\frac{(s - x _{2})^{2}}{2} dx _{2} = \frac{1}{s ^{2}}\left[\frac{s ^{3}}{6}\right] + \frac{1}{s ^{2}} \left[\frac{s ^{3}}{6}\right]\\
	&=\frac{s}{3}
\end{align*}
$$
----------

2. Show that Problem A is equivalent to computing
$$
\begin{equation*}
	I = \frac{1}{\pi ^{2} R ^{4}} \int_{0}^{R}\int_{0}^{R}\int_{0}^{2 \pi}\int_{0}^{2 \pi} \sqrt{r ^{2}_{1} + r ^{2}_{2} - 2 r _{1}r _{2} \cos \phi (\theta _{1}, \theta _{2})}r _{1}r _{2}d \theta _{1}d \theta _{2}dr _{1}dr _{2},
\end{equation*}
$$
where $\phi (\theta _{1}, \theta _{2})$ is the central angle between $r _{1}$ and $r _{2}$.

*Hint: Draw a picture.* 

`Solution`

We want to find $\mathbb{E} [|| p - q||] = \mathbb{E}\left[\sqrt{(p _{x}- q _{x})^{2} + (p _{y} - q _{y})^{2}}\right]$, where $p = (p _{x}, p _{y})$ and $q = (q _{x}, q _{y})$ are uniformly sampled from a disc with radius $R$. 

Note that we can rewrite $p$ and $q$ in polar coordinates:
$$
\begin{align*}
	p &= (p _{x}, p _{y}) = (r _{1}\cos \theta _{1}, r _{1}\sin \theta _{1}) \\
	q &= (q _{x}, q _{y}) = (r _{2}\cos \theta _{2}, r _{2}\sin \theta _{2})
\end{align*}
$$
If that in mind, we can consider using a transformation, by change of variables, to find the expected distance of the desired points.

----------
**Theorem 8.1.7 (Change of Variables)[Blitzstein].**
Let $X = (X _{1},\dots,X _{n})$ be a continuous random vector with joint PDF $f _{X}$. Let $g: A _{0} \to B _{0}$ be an invertible function, where $A _{0}$ and $B _{0}$ are open subsets of $\mathbb{R}^{n}$, $A _{0}$ contains the support of $X$, and $B _{0}$ is the range of $g$.

Let $Y = g (X)$, and mirror this by letting $y = g (x)$. Since $g$ is invertible, we also have $X = g ^{-1}(Y)$ and $x = g ^{-1}(y)$.

Suppose that all the partial derivatives $\frac{\partial x _{i}}{\partial y_{j}}$ exist and are continuous, so we can form the Jacobian matrix
$$
\begin{equation*}
	J =\frac{\partial x}{\partial y} \begin{pmatrix}
		\frac{\partial x _{1}}{\partial y_{1}} & \cdots & \frac{\partial x _{1}}{\partial y_{n}} \\
		\vdots & \ddots & \vdots \\
		\frac{\partial x _{n}}{\partial y_{1}} & \cdots & \frac{\partial x _{n}}{\partial y_{n}}
	\end{pmatrix}
\end{equation*}
$$
Also assume that the determinant of this Jacobian matrix is never 0. Then the joint PDF of $Y$ is 
$$
\begin{equation*}
	f _{Y}(y) = f _{X}(g ^{-1}(y)) \cdot |\det (J)|
\end{equation*}
$$
for $y \in B _{0}$, and 0 otherwise.

----------
We have the following PDF for points $k$ in the uniform disc distribution with radius $R$:
$$
\begin{align*}
	&f(k) = \frac{1}{\pi R^{2}}\mathbb{I}_{\left\{ ||k|| \leq R \right\}}\\
	&\Downarrow \\
	&f _{X,Y}(p _{x},p _{y}) = \frac{1}{\pi R^{2}}\mathbb{I}_{\left\{ \sqrt{p _{x}^{2}+ p _{y}^{2}} \leq R \right\}}\\
\end{align*}
$$
Using the change of variables theorem, we have
$$
\begin{equation*}
	f _{\mathcal{R},\vartheta}(r,\theta) = f _{X,Y}(x,y)\cdot |\det J|
\end{equation*}
$$
where
$$
\begin{equation*}
	J =\frac{\partial (p _{x},p _{y})}{\partial (r,\theta)} =  \begin{pmatrix}
		\cos \theta _{1} &  - r _{1}\sin \theta _{1}\\
		\sin \theta _{1} & 	r _{1}\cos \theta _{1}
		\end{pmatrix}
\end{equation*}
$$
and 
$$
\begin{equation*}
	\det J = r _{1}\cos ^{2} \theta_{1} + r _{1} \sin ^{2} \theta _{1} = r _{1}
\end{equation*}
$$
Thus
$$
\begin{equation*}
	f _{\mathcal{R},\vartheta}(r,\theta) = \frac{1}{\pi R^{2}}\cdot\mathbb{I}_{\left\{0 <r \leq R \right\}}\cdot\mathbb{I}_{\left\{0<\theta \leq 2\pi \right\}}\cdot r
\end{equation*}
$$

Finally, before the calculation of the expectation, we write the distance between $p$ and $q$ in polar coordinates:
$$
\begin{align*}
	||p - q|| &= \sqrt{(p _{x}- q _{x})^{2} + (p _{y} - q _{y})^{2}} = \sqrt{(r _{1}\cos \theta _{1} - r _{2}\cos \theta _{2})^{2} + (r _{1}\sin \theta _{1} - r _{2}\sin \theta _{2})^{2}} \\
	&= \small\sqrt{(r _{1}^{2}\cos ^{2}\theta _{1} + r _{2}\cos ^{2} \theta _{2}- 2r _{1}r _{2}\cos \theta _{1}\theta _{2}) + (r _{1}^{2}\sin ^{2}\theta _{1} + r _{2}\sin ^{2} \theta _{2}- 2r _{1}r _{2}\sin \theta _{1}\theta _{2})} \\
	&= \small\sqrt{(r _{1}^{2} + r _{2}^{2} - 2r _{1}r _{2}\cos (\theta _{1} - \theta _{2}))}, \qquad \text{(since} \cos \alpha \beta + \sin \alpha \beta = \cos (\alpha -\beta)).
\end{align*}
$$
Thus, 
$$
\begin{align*}
	\mathbb{E}_{P,Q}\left[||p - q||\right] &= \int_{\mathbb{R}^{2}}\int_{\mathbb{R}^{2}} ||p - q|| \frac{1}{\pi R ^{2}}\mathbb{I}_{\left\{||p|| \leq R \right\}}\frac{1}{\pi R ^{2}}\mathbb{I}_{\left\{||q||\leq R \right\}} dp dq\\
	&\Downarrow \tiny\text{change of variables} \\
\end{align*}
$$
$$
\begin{align*}
	\scriptsize\mathbb{E}_{\mathcal{R},\vartheta}\left[||p (r _{1},\theta _{1})-q (r _{2}, \theta _{2})|| \right]&\scriptsize= \int_{0}^{2 \pi}\int_{0}^{2 \pi} \int_{0}^{R} \int_{0}^{R} \sqrt{(r _{1}^{2} + r _{2}^{2} - 2r _{1}r _{2}\cos (\theta _{1} - \theta _{2}))} f _{\mathcal{R},\varphi}(r_{1},\theta_{1})f _{\mathcal{R},\varphi}(r_{2},\theta_{2})dr _{1}dr _{2}d\theta _{1}d\theta _{2}\\
	&\scriptsize=  \int_{0}^{2 \pi}\int_{0}^{2 \pi} \int_{0}^{R} \int_{0}^{R} \sqrt{(r _{1}^{2} + r _{2}^{2} - 2r _{1}r _{2}\cos (\theta _{1} - \theta _{2}))} \frac{1}{\pi R ^{2}}r _{1}\frac{1}{\pi R ^{2}}r _{2}dr _{1}dr _{2}d\theta _{1}d\theta _{2}\\
	&\scriptsize= \frac{1}{\pi ^{2} R ^{4}} \int_{0}^{2 \pi}\int_{0}^{2 \pi} \int_{0}^{R} \int_{0}^{R} \sqrt{(r _{1}^{2} + r _{2}^{2} - 2r _{1}r _{2}\cos (\theta _{1} - \theta _{2}))} r _{1}r _{2}dr _{1}dr _{2}d\theta _{1}d\theta _{2}\\
\end{align*}
$$

----------
3. Compute $I$ in closed-form.

*Hint: Look up Crofton's mean value theorem or Crofton's formula.* 

`Solution`

First let's state Crofton's formula and mean value theorem, as described in [2]:

----------
**Theorem 2.2.4. (Crofton's Formula)** 
Let $D$ be a domain in the k-dimensional Euclidean space $\mathbb{R}^{k}$ with the volume content $V$. Consider a small increment in $D$ and let the enlarged domain be $D_{1},D \subset D_{1}$. Let the points $x_{1},\dots,x_{n}$ be independently and identically distributed in $D$. Let $P$ be an invariant probability of an event in $D$. Then
$$
\begin{equation*}
	\frac{dP}{dV} = \frac{n}{V}(P_{1}-P)
\end{equation*}
$$
where $P_{1}$ is the same probability $P$ when exactly one point is on the boundary of $D$ and the remaining $n-1$ points inside $D$.

----------

----------
**Theorem 2.2.5.** 
Let $D,V$ and $x_{1},\dots,x_{n}$ be as defined in Theorem 2.2.4. Let $\mu$ be the invariant expected value of a function of $x_{1},\dots,x_{n}$ when all the points are inside $D$ and $\mu_{1}$, the same $\mu$ when exactly one point is on the boundary an $n-1$ points inside $D$. Then
$$
\begin{equation*}
	\frac{d \mu}{dV} = \frac{n}{V}(\mu_{1}-\mu)
\end{equation*}
$$

----------

## Part II - getting your hands dirty

Now we will move on to implementation.

**Problem B** Employ a simulation algorithm to approximate $I$. Provide point and interval estimates and give theoretical guarantees about them (consistency, coverage, etc).

1. Represent $I$ as $\int_{\mathcal{X}}\phi (x)\pi (x)dx$ and justify your choice of $\phi, \pi$ and $\mathcal{X}$. Recall that these choices are arbitrary up to a point, but they might lead to wildly different empirical performances and theoretical properties for estimators of $I$. Justify your choices in light of the method you have been given to work with. Choose wisely and be rigorous in your justifications.

justify your choice of $\phi, \pi$ and $\mathcal{X}$

`Solution`

Let's first try a simple approuch. By item 2 of Part I, we know that each $p$ draw from a uniform disc can be rewritten in polar coordinates $p = (r \cos \theta, r \sin \theta)$ 
